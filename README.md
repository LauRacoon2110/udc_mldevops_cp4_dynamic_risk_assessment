# Client Attrition Risk Prediction (ML + Flask API)

This project builds and deploys a machine learning pipeline that helps identify corporate clients at risk of attrition. The solution enables a small client management team to focus their efforts on high-risk clients by using an ML-powered risk scoring system, deployed and served through a Flask API.

**GitHub Repository:**  
[github.com/LauRacoon2110/udc_mldevops_cp4_dynamic_risk_assessment](https://github.com/LauRacoon2110/udc_mldevops_cp4_dynamic_risk_assessment)

---

## Project Structure

```
.
â”œâ”€â”€ apicalls.py
â”œâ”€â”€ app.py
â”œâ”€â”€ config.json
â”œâ”€â”€ cronjob.txt
â”œâ”€â”€ deployment.py
â”œâ”€â”€ diagnostics.py
â”œâ”€â”€ fullprocess.py
â”œâ”€â”€ ingestion.py
â”œâ”€â”€ LICENSE
â”œâ”€â”€ mypy.ini
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â”œâ”€â”€ reporting.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ scoring.py
â”œâ”€â”€ training.py
â”œâ”€â”€ utils.py
â”œâ”€â”€ wsgi.py
â”œâ”€â”€ .python-version
â”œâ”€â”€ .pre-commit-config.yaml
â”œâ”€â”€ .gitignore
â”œâ”€â”€ ingesteddata/
â”‚   â”œâ”€â”€ finaldata.csv
â”‚   â””â”€â”€ ingestedfiles.txt
â”œâ”€â”€ models/
â”‚   â””â”€â”€ .gitkeep
â”œâ”€â”€ practicedata/
â”‚   â”œâ”€â”€ dataset1.csv
â”‚   â””â”€â”€ dataset2.csv
â”œâ”€â”€ practicemodels/
â”‚   â”œâ”€â”€ apireturns.txt
â”‚   â”œâ”€â”€ confusionmatrix.png
â”‚   â”œâ”€â”€ latestscore.txt
â”‚   â””â”€â”€ trainedmodel.pkl
â”œâ”€â”€ production_deployment/
â”‚   â”œâ”€â”€ ingestedfiles.txt
â”‚   â”œâ”€â”€ latestscore.txt
â”‚   â””â”€â”€ trainedmodel.pkl
â”œâ”€â”€ screenshots/
â”‚   â””â”€â”€ fullprocess_run_output_prod.png
â”œâ”€â”€ sourcedata/
â”‚   â”œâ”€â”€ dataset3.csv
â”‚   â””â”€â”€ dataset4.csv
â”œâ”€â”€ testdata/
â”‚   â””â”€â”€ testdata.csv
```

---

## Pipeline Steps

This project uses a modular ML pipeline:

1. **Data Ingestion** (`ingestion.py`)  
   Merges new CSVs into a single file (`finaldata.csv`) for model training.

2. **Model Training** (`training.py`)  
   Trains a `LogisticRegression` model using selected features and target.

3. **Model Scoring** (`scoring.py`)  
   Evaluates the trained model using the F1 score.

4. **Model Deployment** (`deployment.py`)  
   Copies model artifacts and logs into the production folder.

5. **Diagnostics** (`diagnostics.py`)  
   Performs:
   - Summary stats
   - Missing value check
   - Execution time tracking
   - Outdated packages scan

6. **Reporting** (`reporting.py`)  
   Creates and saves a confusion matrix plot.

7. **API** (`app.py`)  
   Flask server for exposing prediction and diagnostic endpoints.

8. **Pipeline Automation** (`fullprocess.py`)  
   Executes the entire flow, checks for new data, handles model drift, redeploys models, and triggers reporting and API calls.

---

## Model Details

- **Model**: `LogisticRegression` (scikit-learn)
- **Features**:
  - `lastmonth_activity`
  - `lastyear_activity`
  - `number_of_employees`
- **Target**: `exited` (binary classification)
- **Metric**: `F1 Score`

---

## ðŸ”Œ API Endpoints

| Endpoint         | Method | Description                                       |
|------------------|--------|---------------------------------------------------|
| `/prediction`     | POST   | Predict churn for a given CSV file               |
| `/scoring`        | GET    | Return the model's F1 score                      |
| `/summarystats`   | GET    | Get data mean, median, and standard deviation    |
| `/diagnostics`    | GET    | Check execution time, nulls, outdated packages   |

---

## Manual Execution

### 1. Install dependencies

```bash
pip install -r requirements.txt
```

### 2. Set Production Mode

Before running the a single pipeline step, make sure to check the environment settings `config.json`:
e.g. 

```json
"active_environment": "dev"
```

This ensures the pipeline uses `/sourcedata/`, `/models/`, and `/production_deployment/`.

### 3. Run each pipeline step manually (in dev or prod mode)

```bash
python ingestion.py
python training.py
python scoring.py
python deployment.py
python diagnostics.py
python reporting.py
```

### 4. Run the Flask API

```bash
python app.py
```

Then test it using:

```bash
python apicalls.py
```

---

## Run the Full Pipeline

You can run everything with:

```bash
python fullprocess.py
```

This script:
- Detects new data
- Ingests it
- Checks for model drift
- Retrains and redeploys if needed
- Starts the API and hits endpoints
- Generates a new confusion matrix

---

## Automate with Cron

The pipeline can be triggered every 10 minutes using the following cron job (`cronjob.txt`):

```cron
*/10 * * * * /home/laura/projects/udc_mldevops_cp4_dynamic_risk_assessment/.venv/bin/python /home/laura/projects/udc_mldevops_cp4_dynamic_risk_assessment/fullprocess.py >> /home/laura/projects/udc_mldevops_cp4_dynamic_risk_assessment/fullprocess.log 2>&1
```

---

## Linting & Formatting

Pre-commit tools used:

- `flake8`
- `black`
- `mypy`
- `isort`
- `pytest`

Enable with:

```bash
pre-commit install
pre-commit run --all-files
```

---

## License

This project is licensed under the **GNU General Public License v3.0 (GPL-3.0)**  
See [`LICENSE`](./LICENSE) for more information.
